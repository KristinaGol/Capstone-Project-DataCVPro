{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting skills from job description\n",
    "Function will return nested dict (see NLP_skillNer_example)\n",
    "\n",
    "!NOTE: The extraction is a very long process. 500 jobs can take multiple hours. \n",
    "\n",
    "There are errors in specific rows. Solved beneath \"return None\" and with a \"for loop and continue\" beneath\n",
    "If you want the dictionnaires for each row as a column of the df, run cell below with the function \"skills\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports SKILLNER\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# load default skills data base\n",
    "from skillNer.general_params import SKILL_DB # EMSI skills database \n",
    "# import skill extractor\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "\n",
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np\n",
    "import ast  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"data/df_clean_for_token.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>description</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>as the leader in cloud-managed it, cisco merak...</td>\n",
       "      <td>2023-08-02 03:00:13.054897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>as a senior business analyst you will contribu...</td>\n",
       "      <td>2023-08-02 03:00:13.054897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>overview:\\n\\namyx is seeking to hire a data an...</td>\n",
       "      <td>2023-08-02 03:00:13.054897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i am looking for someone to help me build an a...</td>\n",
       "      <td>2023-08-02 03:00:13.054897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>position vacancy – data analyst to support the...</td>\n",
       "      <td>2023-08-02 03:00:13.054897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                        description  \\\n",
       "0      0  as the leader in cloud-managed it, cisco merak...   \n",
       "1      1  as a senior business analyst you will contribu...   \n",
       "2      2  overview:\\n\\namyx is seeking to hire a data an...   \n",
       "3      3  i am looking for someone to help me build an a...   \n",
       "4      4  position vacancy – data analyst to support the...   \n",
       "\n",
       "                    date_time  \n",
       "0  2023-08-02 03:00:13.054897  \n",
       "1  2023-08-02 03:00:13.054897  \n",
       "2  2023-08-02 03:00:13.054897  \n",
       "3  2023-08-02 03:00:13.054897  \n",
       "4  2023-08-02 03:00:13.054897  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n"
     ]
    }
   ],
   "source": [
    "# NLP + skillner\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# init skill extractor\n",
    "skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that extracts skills. Can be directly used to apply to a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dictionnaires for each row\n",
    "# def skills(text):\n",
    "#     try:\n",
    "#         skills_ex = skill_extractor.annotate(text)\n",
    "#         return(skills_ex)\n",
    "#     except Exception as e:\n",
    "#         return None\n",
    "\n",
    "# # applies the function to the range of rows specified by indexing\n",
    "# df_main['skills_ex'] = df_main['description_cleaned'][0:5].apply(skills) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a list of dictionnaires run the loop. Adjust the index range to fit your needs, but keep in mind how long it takes to process a large number of jobs.\n",
    "The list of dictionaires can then be saved and exported as e.g df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround of skillners bugs by ignoring the errors\n",
    "# looping through rows in df.description_cleaned\n",
    "counter = 0\n",
    "#The list that returns the indexes of the error \"out of range\", e.g., for [0:500] = [146, 171, 247, 249, 274, 285, 286, 355, 357, 438, 499]\n",
    "index_list = [] \n",
    "skills_list = [] \n",
    "\n",
    "\n",
    "for id in df_main['description_cleaned'][30990:31000]: #specify the index range \n",
    "    try:\n",
    "        skills_ex = skill_extractor.annotate(id)\n",
    "        skills_list.append(skills_ex)\n",
    "        counter = counter + 1 #ignores this when fails\n",
    "        print(counter)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry at counter {counter}: {str(e)}\")\n",
    "        index_list.append(counter) # list of indexes that werent processed\n",
    "        counter = counter + 1\n",
    "        continue\n",
    "\n",
    "print(counter)\n",
    "print(len(skills_list))\n",
    "display(skills_list)\n",
    "print(index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two way to work now.\n",
    "1. work directly with skills_list.                              -- marked as WAY 1\n",
    "2. save skills_list and export-import back as e.g dataframe     -- marked as WAY 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WAY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Issues:\n",
    "1. Unhashable Type Error:\n",
    "\n",
    "    The initial error was caused by attempting to use a dictionary as a key in another dictionary (dict_frequency). Dictionaries are mutable objects and not hashable.\n",
    "\n",
    "2. TypeError: Object of type int64 is not JSON serializable:\n",
    "\n",
    "    This issue likely occurred when trying to serialize an int64 object to JSON. The int64 type is specific to NumPy and can cause issues when working with JSON.\n",
    "\n",
    "3. Empty Dictionaries:\n",
    "\n",
    "    Even after resolving the above issues, the dictionaries dict_hard and dict_soft remained empty, which could be due to mismatched keys between skills_list and SKILL_DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAY 1\n",
    "# above code does not summarize skills (eg. we have analytics, analytical & data analysis seperately)\n",
    "# to group them we will access skill_ID from the output dict, count them and translate them back to skill_name from the SKILL_DB library\n",
    "\n",
    "# Extract values with the key 'skill_id' from each dictionary in the skills_list\n",
    "skills_ids = []\n",
    "for d in skills_list:\n",
    "    full_matches = d.get('results', {}).get('full_matches', [])\n",
    "    ngram_scored = d.get('results', {}).get('ngram_scored', [])\n",
    "    for item in full_matches:\n",
    "        skills_ids.append(item.get('skill_id'))\n",
    "    for item in ngram_scored:\n",
    "        skills_ids.append(item.get('skill_id'))\n",
    "        \n",
    "print(skills_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# getting frequency of our skills from skills_list that utilized skill_id\n",
    "dict_frequency = {}\n",
    "for i in skills_list:\n",
    "    # Convert the dictionary to a JSON string using the custom encoder\n",
    "    i_json = json.dumps(i, sort_keys=True, cls=NumpyEncoder)\n",
    "    \n",
    "    if i_json in dict_frequency:\n",
    "        dict_frequency[i_json] += 1\n",
    "    else:\n",
    "        dict_frequency[i_json] = 1\n",
    "\n",
    "print(dict_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dictionaries to store counts of hard and soft skills\n",
    "dict_hard = {}\n",
    "dict_soft = {}\n",
    "\n",
    "# Iterate over each item in the skills_list\n",
    "for item in skills_list:\n",
    "    # Extract the 'full_matches' list from the 'results' dictionary, default to an empty list if not present\n",
    "    full_matches = item.get('results', {}).get('full_matches', [])\n",
    "    \n",
    "    # Iterate over each match in the 'full_matches' list\n",
    "    for match in full_matches:\n",
    "        # Extract the 'skill_id' from the match\n",
    "        skill_id = match.get('skill_id')\n",
    "        \n",
    "        # Check if 'skill_id' exists\n",
    "        if skill_id:\n",
    "            # Retrieve 'skill_type' and 'skill_name' from SKILL_DB using 'skill_id'\n",
    "            skill_type = SKILL_DB.get(skill_id, {}).get('skill_type')\n",
    "            skill_name = SKILL_DB.get(skill_id, {}).get('skill_name')\n",
    "            \n",
    "            # Check if 'skill_type' is 'Hard Skill' and 'skill_name' exists\n",
    "            if skill_type == 'Hard Skill' and skill_name:\n",
    "                # Update count in 'dict_hard'\n",
    "                dict_hard[skill_name] = dict_hard.get(skill_name, 0) + 1\n",
    "            # If not a 'Hard Skill' and 'skill_name' exists\n",
    "            elif skill_name:\n",
    "                # Update count in 'dict_soft'\n",
    "                dict_soft[skill_name] = dict_soft.get(skill_name, 0) + 1\n",
    "\n",
    "# Sort dictionaries in descending order by count\n",
    "sorted_dict_soft = dict(sorted(dict_soft.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_dict_hard = dict(sorted(dict_hard.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Display sorted dictionaries\n",
    "display(sorted_dict_soft, sorted_dict_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WAY 2 - import skills_set saved as dataframe from csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "# As going through job descriptions takes much time, it is better to split across multiple data frames\n",
    "\n",
    "#skills_list_df = pd.DataFrame(skills_list)\n",
    "#skills_list_df.to_csv(\"data/skills_list_31500_32022.csv\") # index_list [18,19]\n",
    "#skills_list_df.to_csv(\"data/skills_list_31000_31500.csv\") # index list empty\n",
    "#skills_list_df.to_csv(\"data/skills_list_29500_31000.csv\") #[2, 98, 103, 261, 262, 282, 311, 448, 526, 758, 759, 883, 911, 926, 943, 955, 963, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import back\n",
    "SKILLS1 = pd.read_csv(\"data/skills_list_31500_32022.csv\")\n",
    "SKILLS2 = pd.read_csv(\"data/skills_list_31000_31500.csv\")# check me later! repeating text \"position summary what you ...\"\"\n",
    "SKILLS3 = pd.read_csv(\"data/skills_list_29500_31000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data frames to one in order to extract the skills\n",
    "SKILLS = pd.concat([SKILLS1, SKILLS2, SKILLS3], ignore_index=True)\n",
    "SKILLS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast needed\n",
    "# Read the CSV file, specifying that 'results' should be treated as a literal_eval\n",
    "\n",
    "# Initialize an empty list to store skill_ids\n",
    "skills_ids = []\n",
    "\n",
    "# Iterate over each row in SKILLS DataFrame\n",
    "for _, row in SKILLS.iterrows():\n",
    "    # Safely evaluate the literals in 'results' column using ast.literal_eval\n",
    "    results_dict = ast.literal_eval(row['results'])\n",
    "    \n",
    "    # Extract 'full_matches' and 'ngram_scored' from the evaluated dictionary\n",
    "    full_matches = results_dict.get('full_matches', [])\n",
    "    ngram_scored = results_dict.get('ngram_scored', [])\n",
    "    \n",
    "    # Iterate over full_matches\n",
    "    for item in full_matches:\n",
    "        skills_ids.append(item.get('skill_id'))\n",
    "    \n",
    "    # Iterate over ngram_scored\n",
    "    for item in ngram_scored:\n",
    "        skills_ids.append(item.get('skill_id'))\n",
    "\n",
    "# Display the first few skill_ids\n",
    "print(skills_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# Getting frequency of skills from SKILLS DataFrame that utilized skill_id\n",
    "dict_frequency = {}\n",
    "\n",
    "# Iterate over each row in SKILLS DataFrame\n",
    "for _, row in SKILLS.iterrows():\n",
    "    # Safely evaluate the literals in 'results' column using ast.literal_eval\n",
    "    results_dict = ast.literal_eval(row['results'])\n",
    "    \n",
    "    # Convert the dictionary to a JSON string using the custom encoder\n",
    "    results_json_str = json.dumps(results_dict, sort_keys=True, cls=NumpyEncoder)\n",
    "    \n",
    "    if results_json_str in dict_frequency:\n",
    "        dict_frequency[results_json_str] = dict_frequency[results_json_str] + 1\n",
    "    else:\n",
    "        dict_frequency[results_json_str] = 1\n",
    "\n",
    "display(dict_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dictionaries to store counts of hard and soft skills\n",
    "dict_hard = {}\n",
    "dict_soft = {}\n",
    "\n",
    "# Iterate over each row in the SKILLS DataFrame\n",
    "for _, row in SKILLS.iterrows():\n",
    "    # Safely evaluate the literals in 'results' column using ast.literal_eval\n",
    "    results_dict = ast.literal_eval(row['results'])\n",
    "    \n",
    "    # Extract the 'full_matches' list from the 'results' dictionary, default to an empty list if not present\n",
    "    full_matches = results_dict.get('full_matches', [])\n",
    "    \n",
    "    # Iterate over each match in the 'full_matches' list\n",
    "    for match in full_matches:\n",
    "        # Extract the 'skill_id' from the match\n",
    "        skill_id = match.get('skill_id')\n",
    "        \n",
    "        # Check if 'skill_id' exists\n",
    "        if skill_id:\n",
    "            # Retrieve 'skill_type' and 'skill_name' from SKILL_DB using 'skill_id'\n",
    "            skill_type = SKILL_DB.get(skill_id, {}).get('skill_type')\n",
    "            skill_name = SKILL_DB.get(skill_id, {}).get('skill_name')\n",
    "            \n",
    "            # Check if 'skill_type' is 'Hard Skill' and 'skill_name' exists\n",
    "            if skill_type == 'Hard Skill' and skill_name:\n",
    "                # Update count in 'dict_hard'\n",
    "                dict_hard[skill_name] = dict_hard.get(skill_name, 0) + 1\n",
    "            # If not a 'Hard Skill' and 'skill_name' exists\n",
    "            elif skill_name:\n",
    "                # Update count in 'dict_soft'\n",
    "                dict_soft[skill_name] = dict_soft.get(skill_name, 0) + 1\n",
    "\n",
    "# Sort dictionaries in descending order by count\n",
    "sorted_dict_soft = dict(sorted(dict_soft.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_dict_hard = dict(sorted(dict_hard.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Display sorted dictionaries\n",
    "display(sorted_dict_soft, sorted_dict_hard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a skill list based on the extracted skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add info hard/ soft skill to skill count data\n",
    "# first convert to DataFrame:\n",
    "df_soft = pd.DataFrame(list(sorted_dict_soft.items()), columns=['skill', 'count'])\n",
    "df_hard = pd.DataFrame(list(sorted_dict_hard.items()), columns=['skill', 'count'])\n",
    "df_soft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add type of skill inside the data frames in order to concat\n",
    "df_soft['type'] = 'Soft Skill'\n",
    "df_hard['type'] = 'Hard Skill'\n",
    "\n",
    "# concat df_soft & df_hard to create one complete dataframe\n",
    "df_type = pd.concat([df_hard, df_soft])\n",
    "df_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type[\"type\"].value_counts() # there are ca. 16+ x more hard skills than soft skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas for later, not relevant currently\n",
    "# preparing for nlp stemmer that has problems when there are multiple words per entry\n",
    "df_type_brackets = df_type[\"skill\"].str.contains(\"\\(\")\n",
    "df_type2 = df_type[df_type_brackets]\n",
    "display(df_type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas for later, not relevant currently\n",
    "# splitting strings into seperate words in skill\n",
    "df_type['word_list'] = df_type['skill'].str.split()\n",
    "display(df_type[12:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed skills that appeared only once\n",
    "df_skills_top = df_type.query('count >= 2').copy()\n",
    "#drop count because this the overall count that includes multiple mentions of the same skill inside the same job description\n",
    "df_skills_top.drop(['count'], axis=1, inplace=True)\n",
    "# remove brackets and abbreviations inside\n",
    "df_skills_top['skill_clean'] = df_skills_top['skill'].str.split('(').str[0] \n",
    "# convert to lower case as description is also lower case\n",
    "df_skills_top['skill_clean'] = df_skills_top['skill_clean'].apply(lambda x: x.lower()) \n",
    "\n",
    "display(df_skills_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skills_top['skill_clean'] = df_skills_top['skill_clean'].str.replace('s$', '', regex=True)\n",
    "df_skills_top[df_skills_top['skill_clean'].str.contains('communicat', case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is not finished yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a skill list through job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this section we utilze a skill list that was not extracted above, but from a different data set\n",
    "# later that skill list and the skill list produced in this notebook will be combined\n",
    "# import the \"other\" skills list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of all skills\n",
    "skill_list_top = df_top_skills.skill.tolist()\n",
    "\n",
    "# #stammer\n",
    "ps = PorterStemmer()\n",
    "  \n",
    "for w in skill_list_top:\n",
    "    print(w, \" : \", ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading\n",
    "# load to database\n",
    "# df:exp1 as \"jobs_current_skills_timeline\"\n",
    "#df_skill_count as \"skill_count_current\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# write dataset into database\n",
    "\n",
    "# Import get_engine from sql_functions.py. You will need to restart your kernel and rerun at this point since we changed the module since we first imported it.\n",
    "from sql_functions import get_engine\n",
    "# create a variable called engine using the get_engine function\n",
    "engine = get_engine()\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "table_name = \"skill_count_current\"\n",
    "schema = 'capstone_datacvpro'\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        df_skill_count.to_sql(name=table_name, # Name of SQL table variable  \n",
    "                        con=engine, # Engine or connection\n",
    "                        schema=schema, # your class schema variable\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_name} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None\n",
    "else:\n",
    "    print('No engine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_base_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
