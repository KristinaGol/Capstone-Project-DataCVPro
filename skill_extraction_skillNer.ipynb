{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy \n",
    "import sql_functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jobs data\n",
    "\n",
    "schema = 'capstone_datacvpro'\n",
    "\n",
    "jobs_20 = sf.get_dataframe(f' SELECT * FROM {schema}.analysts_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n"
     ]
    }
   ],
   "source": [
    "# skill extraction with skillNer\n",
    "\n",
    "import en_core_web_lg\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# import skill extractor\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "from skillNer.general_params import SKILL_DB\n",
    "\n",
    "# init params of skill extractor\n",
    "nlp = en_core_web_lg.load()\n",
    "# init skill extractor\n",
    "skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all special characters to prevent errors\n",
    "jobs_20['job_description_mod'] = jobs_20['job_description'].str.replace(\"’\",\" \").str.replace(\"/\",\" \").str.replace(\"§\",\" \").str.replace(\"·\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nf_base_capstone/lib/python3.9/site-packages/skillNer/utils.py:99: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  vec_similarity = token1.similarity(token2)\n"
     ]
    }
   ],
   "source": [
    "# extracting skills from job description - will return nested dict (see NLP_skillNer_example)\n",
    "# this will create new column with skills_ex output\n",
    "\n",
    "def skills(text):\n",
    "    index_list = []\n",
    "    try:\n",
    "        skills_ex = skill_extractor.annotate(text)\n",
    "        return(skills_ex)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# apply to column\n",
    "jobs_20['skills_ex'] = jobs_20['job_description'][232:238].apply(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nf_base_capstone/lib/python3.9/site-packages/skillNer/utils.py:99: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  vec_similarity = token1.similarity(token2)\n"
     ]
    }
   ],
   "source": [
    "#looping through rows in job_description\n",
    "# this package cant  handle special characters (', /, ..), replace in new column job_description_mod\n",
    "#rewrote the code to ignore and skip entries that produce errors\n",
    "\n",
    "counter = 0\n",
    "index_list = []\n",
    "skills_list = []\n",
    "\n",
    "for id in jobs_20['job_description_mod']:\n",
    "    try:\n",
    "        skills_ex = skill_extractor.annotate(id)\n",
    "        skills_list.append(skills_ex)\n",
    "        counter = counter + 1 #ignores this when fails\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry at counter {counter}: {str(e)}\")\n",
    "        index_list.append(counter)\n",
    "        counter = counter + 1\n",
    "        continue\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values with the key 'doc_node_value' from each dictionary in the skills_list\n",
    "skills_values = []\n",
    "for d in skills_list:\n",
    "    full_matches = d.get('results', {}).get('full_matches', [])\n",
    "    ngram_scored = d.get('results', {}).get('ngram_scored', [])\n",
    "    for item in full_matches:\n",
    "        skills_values.append(item.get('doc_node_value'))\n",
    "    for item in ngram_scored:\n",
    "        skills_values.append(item.get('doc_node_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count most occuring skills\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of each skill\n",
    "skill_counts = Counter(skills_values)\n",
    "\n",
    "# Get the most common skills\n",
    "most_common_skills = skill_counts.most_common(50)\n",
    "\n",
    "# Print the most common skills\n",
    "for skill, count in most_common_skills:\n",
    "    print(f'{skill}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above code does not summarize skills (eg. we have analytics, analytical & data analysis seperately)\n",
    "# to group them we will access skill_ID from the output dict, count them and translate them back to skill_name from the SKILL_DB library\n",
    "\n",
    "# Extract values with the key 'skill_id' from each dictionary in the skills_list\n",
    "skills_ids = []\n",
    "for d in skills_list:\n",
    "    full_matches = d.get('results', {}).get('full_matches', [])\n",
    "    ngram_scored = d.get('results', {}).get('ngram_scored', [])\n",
    "    for item in full_matches:\n",
    "        skills_ids.append(item.get('skill_id'))\n",
    "    for item in ngram_scored:\n",
    "        skills_ids.append(item.get('skill_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict with count of skill IDs\n",
    "\n",
    "dict_count = {}\n",
    "for i in skills_ids:\n",
    "    if i in dict_count:\n",
    "        dict_count[i] = dict_count[i] + 1\n",
    "    else:\n",
    "        dict_count[i] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert IDs to skill names using SKILL_DB library\n",
    "\n",
    "dict_name = {}\n",
    "for key, value in dict_count.items():\n",
    "    key_temp = SKILL_DB[key][\"skill_name\"]\n",
    "    dict_name[key_temp] = dict_count[key]\n",
    "\n",
    "sorted_dict_name = dict(sorted(dict_name.items(), key=lambda item: item[1], reverse=True))\n",
    "display(sorted_dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinguish soft & hard skills\n",
    "\n",
    "dict_name = {}\n",
    "dict_soft = {}\n",
    "for key, value in dict_count.items():\n",
    "    if SKILL_DB[key][\"skill_type\"] == \"Hard Skill\":\n",
    "        key_temp = SKILL_DB[key][\"skill_name\"]\n",
    "        dict_name[key_temp] = dict_count[key]\n",
    "    else:\n",
    "        key_temp = SKILL_DB[key][\"skill_name\"]\n",
    "        dict_soft[key_temp] = dict_count[key]\n",
    "        \n",
    "sorted_dict_soft = dict(sorted(dict_soft.items(), key = lambda item: item[1], reverse=True)) # = dict(...)\n",
    "display(dict_name, sorted_dict_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DataFrame:\n",
    "\n",
    "df_soft = pd.DataFrame(list(sorted_dict_soft.items()), columns=['skill', 'count'])\n",
    "df_hard = pd.DataFrame(list(dict_name.items()), columns=['skill', 'count'])\n",
    "df_soft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column 'type'\n",
    "\n",
    "df_soft['type'] = 'Soft Skill'\n",
    "df_hard['type'] = 'Hard Skill'\n",
    "\n",
    "# concat to create complete df\n",
    "\n",
    "df_skills = pd.concat([df_hard, df_soft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to database\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# write dataset into database\n",
    "\n",
    "# Import get_engine from sql_functions.py. You will need to restart your kernel and rerun at this point since we changed the module since we first imported it.\n",
    "from sql_functions import get_engine\n",
    "\n",
    "# create a variable called engine using the get_engine function\n",
    "engine = get_engine()\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "table_name = 'skills_20'\n",
    "schema = 'capstone_datacvpro'\n",
    "\n",
    "# Write records stored in a dataframe to SQL database\n",
    "if engine!=None:\n",
    "    try:\n",
    "        df_skills.to_sql(name=table_name, # Name of SQL table variable\n",
    "                        con=engine, # Engine or connection\n",
    "                        schema=schema, # your class schema variable\n",
    "                        if_exists='replace', # Drop the table before inserting new values \n",
    "                        index=False, # Write DataFrame index as a column\n",
    "                        chunksize=5000, # Specify the number of rows in each batch to be written at a time\n",
    "                        method='multi') # Pass multiple values in a single INSERT clause\n",
    "        print(f\"The {table_name} table was imported successfully.\")\n",
    "    # Error handling\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        engine = None\n",
    "else:\n",
    "    print('No engine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_base_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
