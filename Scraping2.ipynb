{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # http requests\n",
    "from bs4 import BeautifulSoup # Webscrape\n",
    "from collections import defaultdict # Default dictionary: store a list with each key\n",
    "import pandas as pd     # DF\n",
    "from parse import parse\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Check my video webscrape Indeed June 2022:`\n",
    "\n",
    "https://youtu.be/-SjrfjKJqqI\n",
    "\n",
    "\n",
    "# `Indeed NLP job descriptions video:`\n",
    "\n",
    "https://youtu.be/bmetLE2L00U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.indeed.co.in/jobs?q=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m skill \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&l=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m place \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&sort=date\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&start=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(page \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Get request to indeed with headers above (you don't need headers!)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39m\u001b[43mheaders\u001b[49m)\n\u001b[1;32m     20\u001b[0m html \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Scrapping the Web (you can use 'html' or 'lxml')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "# this was used for the person contacting me who had these details for their system\n",
    "#headers = {\n",
    " #   \"User-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
    "\n",
    "# Skills & Place of Work\n",
    "skill = input('Enter your Skill: ').strip()\n",
    "place = input('Enter the location: ').strip()\n",
    "no_of_pages = int(input('Enter the # of pages to scrape: '))\n",
    "\n",
    "indeed_posts=[]\n",
    "\n",
    "for page in range(no_of_pages):\n",
    "    \n",
    "    # Connecting to India Indeed\n",
    "        url = 'https://www.indeed.co.in/jobs?q=' + skill + \\\n",
    "            '&l=' + place + '&sort=date' +'&start='+ str(page * 10)\n",
    "        \n",
    "        # Get request to indeed with headers above (you don't need headers!)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        html = response.text\n",
    "\n",
    "        # Scrapping the Web (you can use 'html' or 'lxml')\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Outer Most Entry Point of HTML:\n",
    "        outer_most_point=soup.find('div',attrs={'id': 'mosaic-provider-jobcards'})\n",
    "        \n",
    "        # \"UL\" lists where the data are stored:\n",
    "        \n",
    "        for i in outer_most_point.find('ul'):\n",
    "            \n",
    "        # Job Title:\n",
    "        \n",
    "            job_title=i.find('h2',{'class':'jobTitle jobTitle-color-purple jobTitle-newJob'})\n",
    "#             print(job_title)\n",
    "            if job_title != None:\n",
    "                jobs=job_title.find('a').text\n",
    "\n",
    "        # Company Name:\n",
    "    \n",
    "            if i.find('span',{'class':'companyName'}) != None:\n",
    "                company=i.find('span',{'class':'companyName'}).text   \n",
    "                \n",
    "        # Links: these Href links will take us to full job description\n",
    "        \n",
    "            if i.find('a') != None:\n",
    "                links=i.find('a',{'class':'jcs-JobTitle'})['href']\n",
    "                \n",
    "        # Salary if available:\n",
    "        \n",
    "            if i.find('div',{'class':'attribute_snippet'}) != None:\n",
    "                salary=i.find('div',{'class':'attribute_snippet'}).text\n",
    "\n",
    "            else:\n",
    "                salary='No Salary'\n",
    "\n",
    "        # Job Post Date:\n",
    "\n",
    "            if i.find('span', attrs={'class': 'date'}) != None:\n",
    "                post_date = i.find('span', attrs={'class': 'date'}).text\n",
    "\n",
    "        # Put everything together in a list of lists for the default dictionary\n",
    "                        \n",
    "            indeed_posts.append([company,jobs,links,salary, post_date])\n",
    "            \n",
    "            \n",
    "# put together in list\n",
    "\n",
    "# (create a dictionary with keys and a list of values from above \"indeed_posts\")\n",
    "\n",
    "indeed_dict_list=defaultdict(list)\n",
    "\n",
    "# Fields for our DF \n",
    "\n",
    "indeed_spec=['Company','job','link','Salary','Job_Posted_Date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of lists containing jobs with (company,job_title, href_link,salary,when job posted)\n",
    "indeed_posts[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('These Href links will go to a new page containing full job description')\n",
    "print('\\n')\n",
    "print(pd.DataFrame(indeed_posts,columns=indeed_spec)['link'][0]) \n",
    "#these are not the same, probably from recruiter(s)\n",
    "print(pd.DataFrame(indeed_posts,columns=indeed_spec)['link'][1])\n",
    "\n",
    "pd.DataFrame(indeed_posts,columns=indeed_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have to take these links\n",
    "\n",
    "+ These links will provide us with a popup page. The new page that will open will allow us to scrape anything. In our case it is beneficial to grab the job description for each posting in order to use this for NLP later featuring NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all qualification page text: key=index, value=string of text for qualification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descr_txt=[]\n",
    "\n",
    "# Indeed DF with columns we made above and the stored data from scraping\n",
    "Indeed_DF = pd.DataFrame(indeed_posts,columns=indeed_spec)\n",
    "\n",
    "# convert Series to lists of strings\n",
    "my_super_fun_Indeed_links = list(Indeed_DF['link'])\n",
    "\n",
    "# iterator will be our index value for default_dict_list\n",
    "for i in range(len(my_super_fun_Indeed_links)):\n",
    "    \n",
    "    url_href='https://in.indeed.com' + my_super_fun_Indeed_links[i]\n",
    "#     print(url_href)\n",
    "    response = requests.get(url_href, headers=headers)\n",
    "    html_ = response.text\n",
    "    soup_ = BeautifulSoup(html_, 'lxml')\n",
    "    \n",
    "    for ii in soup_.find('div',{'class':'jobsearch-jobDescriptionText'}):\n",
    "        try:\n",
    "            job_descr_txt.append([i,''.join(ii.text.strip())])\n",
    "        except AttributeError:\n",
    "            job_descr_txt.append([i,''])\n",
    "job_descr_txt\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://in.indeed.com/viewjob?cmp=Alpha-Net-Technologies-Pvt-Ltd&t=Database+Developer+Analyst&jk=a5063bed6e53cb53&vjs=3\n",
    "\n",
    "# https://in.indeed.com/viewjob?q=/company/Alpha-Net-Technologies-Pvt-Ltd/jobs/Database-Developer-Analyst-a5063bed6e53cb53?fccid=0e8473839d79bdf1&vjs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with values as lists\n",
    "dct_lst= defaultdict(list)\n",
    "\n",
    "for i in job_descr_txt:\n",
    "    #key value pairs for default_dict_list\n",
    "    dct_lst[i[0]].append(i[1])\n",
    "    \n",
    "dict_lst_jobsDescr=[]\n",
    "\n",
    "for i in dct_lst.values(): # string join: lists of lists of strings\n",
    "    dict_lst_jobsDescr.append(''.join(i))\n",
    "    \n",
    "dict_lst_jobsDescr[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indeed_DF['text_descrption'] = dict_lst_jobsDescr\n",
    "Indeed_DF.head()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Now, the choice is yours to do further analysis!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Thank you for watching:`\n",
    "\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "# <font color=red> Like , Share & SUBscribe</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
